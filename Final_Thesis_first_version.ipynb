{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRo7sGOicPme",
        "outputId": "7d1aa623-b031-43bb-dcd4-c0dc77220dcc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install scipy torch torchaudio museval\r\n",
        "!git clone https://github.com/sigsep/open-unmix-pytorch.git\r\n",
        "%cd /content/open-unmix-pytorch/scripts\r\n",
        "!pip install -r requirements.txt\r\n",
        "!pip install pydub\r\n",
        "!pip install openunmix\r\n",
        "!pip install pyloudnorm\r\n",
        "!pip install noisereduce\r\n",
        "!apt-get install -y sox libsox-dev libsox-fmt-all\r\n",
        "!pip install scipy==1.9.3\r\n",
        "!pip install nussl\r\n",
        "!pip install datasets\r\n",
        "!pip install demucs"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cPK0oxkrvQyn",
        "outputId": "ddf23033-daee-491f-b2cf-4790e66b1783"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "import shutil\r\n",
        "import random\r\n",
        "import librosa\r\n",
        "import soundfile as sf\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import librosa.display\r\n",
        "import scipy.signal\r\n",
        "import noisereduce as nr\r\n",
        "import IPython.display as ipd\r\n",
        "import nussl\r\n",
        "from openunmix import predict, utils"
      ],
      "outputs": [],
      "metadata": {
        "id": "cKtIZ4levlxs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# ======================== Organize Drum Loops ======================== #\r\n",
        "\r\n",
        "# Define paths\r\n",
        "drum_loops_path = \"/content/drive/MyDrive/Drum_Loops\"\r\n",
        "output_path = \"/content/train_data\"\r\n",
        "train_path, valid_path = os.path.join(output_path, \"train\"), os.path.join(output_path, \"valid\")\r\n",
        "\r\n",
        "# Drum loop categories\r\n",
        "categories = {\r\n",
        "    \"kick\": [\"kick\", \"Kick\"],\r\n",
        "    \"hihat\": [\"hihat\", \"Hi-Hat\", \"hats\"],\r\n",
        "    \"clap\": [\"clap\", \"Claps\", \"Clap_Loop\"],\r\n",
        "    \"percussion\": [\"percu\", \"Percu\"]\r\n",
        "}\r\n",
        "\r\n",
        "# Create categorized subfolders\r\n",
        "for category in categories.keys():\r\n",
        "    os.makedirs(os.path.join(drum_loops_path, category), exist_ok=True)\r\n",
        "\r\n",
        "# Move files into their respective folders\r\n",
        "for file in os.listdir(drum_loops_path):\r\n",
        "    file_path = os.path.join(drum_loops_path, file)\r\n",
        "    if os.path.isfile(file_path):  # Only move files, not directories\r\n",
        "        for category, keywords in categories.items():\r\n",
        "            if any(keyword.lower() in file.lower() for keyword in keywords):\r\n",
        "                shutil.move(file_path, os.path.join(drum_loops_path, category, file))\r\n",
        "                print(f\" Moved {file} to {category}/\")\r\n",
        "\r\n",
        "print(\"\\n All drum loops are now correctly categorized!\")\r\n",
        "\r\n",
        "# ==================== Generate Synthetic Mixtures ==================== #\r\n",
        "\r\n",
        "# Create necessary directories\r\n",
        "os.makedirs(train_path, exist_ok=True)\r\n",
        "os.makedirs(valid_path, exist_ok=True)\r\n",
        "\r\n",
        "# Load available drum stems\r\n",
        "kick_files = librosa.util.find_files(os.path.join(drum_loops_path, \"kick\"))\r\n",
        "hihat_files = librosa.util.find_files(os.path.join(drum_loops_path, \"hihat\"))\r\n",
        "clap_files = librosa.util.find_files(os.path.join(drum_loops_path, \"clap\"))\r\n",
        "perc_files = librosa.util.find_files(os.path.join(drum_loops_path, \"percussion\"))\r\n",
        "\r\n",
        "# Ensure we have files for each category\r\n",
        "assert kick_files and hihat_files and clap_files and perc_files, \" Some drum categories are missing!\"\r\n",
        "\r\n",
        "# STFT parameters (matching Open-Unmix)\r\n",
        "n_fft, hop_length = 4096, 1024\r\n",
        "\r\n",
        "# Generate 100 random drum mixtures (90% train, 10% validation)\r\n",
        "num_samples, train_size = 100, int(100 * 0.9)\r\n",
        "\r\n",
        "for i in range(1, num_samples + 1):\r\n",
        "    # Randomly select drum stems\r\n",
        "    kick, _ = librosa.load(random.choice(kick_files), sr=44100)\r\n",
        "    hihat, _ = librosa.load(random.choice(hihat_files), sr=44100)\r\n",
        "    clap, _ = librosa.load(random.choice(clap_files), sr=44100)\r\n",
        "    perc, _ = librosa.load(random.choice(perc_files), sr=44100)\r\n",
        "\r\n",
        "    # Pad and normalize to the longest sample\r\n",
        "    max_len = max(map(len, [kick, hihat, clap, perc]))\r\n",
        "    kick, hihat, clap, perc = (np.pad(s, (0, max_len - len(s))) for s in [kick, hihat, clap, perc])\r\n",
        "\r\n",
        "    # Create synthetic mixture\r\n",
        "    mixture = 0.4 * kick + 0.3 * hihat + 0.5 * clap + 0.3 * perc\r\n",
        "    mixture /= np.max(np.abs(mixture))  # Normalize\r\n",
        "\r\n",
        "    # Normalize \"clap.wav\" separately\r\n",
        "    clap /= np.max(np.abs(clap))\r\n",
        "\r\n",
        "    # Assign to train/valid split\r\n",
        "    dataset_folder = train_path if i <= train_size else valid_path\r\n",
        "    track_folder = os.path.join(dataset_folder, str(i))\r\n",
        "    os.makedirs(track_folder, exist_ok=True)\r\n",
        "\r\n",
        "    # Save mixture and target\r\n",
        "    sf.write(os.path.join(track_folder, \"mixture.wav\"), mixture, 44100)\r\n",
        "    sf.write(os.path.join(track_folder, \"clap.wav\"), clap, 44100)\r\n",
        "\r\n",
        "    print(f\" Created {track_folder}/mixture.wav & {track_folder}/clap.wav\")\r\n",
        "\r\n",
        "print(\"\\n **100 synthetic drum mixtures generated with `nfft=4096` and `hop_length=1024`!**\")\r\n",
        "\r\n",
        "# ====================== Convert Audio to Stereo ====================== #\r\n",
        "\r\n",
        "def convert_to_stereo(directory):\r\n",
        "    \"\"\"Convert mono WAV files to stereo for model compatibility.\"\"\"\r\n",
        "    for root, _, files in os.walk(directory):\r\n",
        "        for file in files:\r\n",
        "            if file.endswith(\".wav\"):\r\n",
        "                file_path = os.path.join(root, file)\r\n",
        "                y, sr = librosa.load(file_path, sr=44100, mono=False)\r\n",
        "\r\n",
        "                if len(y.shape) == 1:  # Convert mono to stereo\r\n",
        "                    y = np.vstack([y, y])\r\n",
        "\r\n",
        "                sf.write(file_path, y.T, sr)\r\n",
        "                print(f\" Converted {file} to stereo (2 channels)\")\r\n",
        "\r\n",
        "convert_to_stereo(output_path)\r\n",
        "print(\" All dataset audio files are now stereo!\")\r\n",
        "\r\n",
        "# ===================== Compute STFT for Debugging ===================== #\r\n",
        "\r\n",
        "sample_file = os.path.join(train_path, \"1/mixture.wav\")\r\n",
        "y, sr = librosa.load(sample_file, sr=44100)\r\n",
        "stft = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\r\n",
        "magnitude = np.abs(stft)\r\n",
        "\r\n",
        "print(f\" Spectrogram Shape: {magnitude.shape}\")  # (freq_bins, time_frames)\r\n",
        "print(f\" Frequency bins: {magnitude.shape[0]}\")\r\n",
        "print(f\" Time frames: {magnitude.shape[1]}\")\r\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zm1h40ZWvYMo",
        "outputId": "57d91a00-2b5d-49a6-fbf7-aa42868292f0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!python train.py \\\n",
        "    --dataset aligned \\\n",
        "    --root /content/train_data \\\n",
        "    --input-file mixture.wav \\\n",
        "    --output-file clap.wav \\\n",
        "    --seq-dur 7.74 \\\n",
        "    --nfft 2048 \\\n",
        "    --nhop 1024 \\\n",
        "    --epochs 15 \\\n",
        "    --batch-size 12"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WxQ0Yocvaj7",
        "outputId": "46ba0f75-88c8-46eb-c282-f249f3296a69"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Load mixture audio\n",
        "mixture_path = \"/content/train_data/valid/99/mixture.wav\"\n",
        "y, sr = librosa.load(mixture_path, sr=44100, mono=False)\n",
        "\n",
        "# Convert to PyTorch tensor and move to the correct device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "audio_tensor = torch.tensor(y, dtype=torch.float32).to(device)\n",
        "\n",
        "# Ensure correct paths\n",
        "model_dir = \"/content/open-unmix-pytorch/scripts/open-unmix/\"  # Directory containing clap.pth and clap.json\n",
        "model_name = \"clap\"  # The target you trained on\n",
        "\n",
        "# 🔍 DEBUG: Check Model Availability\n",
        "print(f\" Loading separator for model: {model_name} on {device}\")\n",
        "\n",
        "separator = utils.load_separator(\n",
        "    model_str_or_path=model_dir,\n",
        "    targets=[model_name],\n",
        "    niter=1,\n",
        "    residual=True,  # Fix: Enable residual to avoid single target issues\n",
        "    wiener_win_len=300,\n",
        "    device=device,\n",
        "    pretrained=True,\n",
        "    filterbank=\"torch\",\n",
        ")\n",
        "\n",
        "separator.freeze()\n",
        "separator.to(device)  # Move the separator model to the correct device\n",
        "print(f\" Separator loaded on {device}: {separator}\")\n",
        "\n",
        "# Run separation manually, ensuring everything is on the same device\n",
        "estimates = predict.separate(\n",
        "    audio=audio_tensor,  # Now on the correct device\n",
        "    rate=sr,\n",
        "    separator=separator,  # Directly use loaded separator\n",
        "    targets=[model_name],\n",
        ")\n",
        "\n",
        "# Check if the model produced an estimate\n",
        "if model_name not in estimates:\n",
        "    print(f\"No '{model_name}' estimate was generated!\")\n",
        "else:\n",
        "    # Extract the separated clap sound and remove extra dimensions\n",
        "    clap_estimate = np.squeeze(estimates[model_name].cpu().numpy())  # Remove unnecessary dimensions\n",
        "\n",
        "    # Save the separated claps\n",
        "    output_path = f\"/content/train_data/valid/99/{model_name}_estimate.wav\"\n",
        "    sf.write(output_path, clap_estimate.T, sr)  # Now correctly shaped\n",
        "\n",
        "    print(f\" Separated {model_name} saved at: {output_path}\")\n",
        "\n",
        "    # Play the estimated clap audio\n",
        "    import IPython.display as ipd\n",
        "    print(f\"\\n Separated {model_name} (Model Output):\")\n",
        "    ipd.display(ipd.Audio(output_path, rate=sr))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "mM1hPR6_vb4F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Define file paths\n",
        "input_wav = \"/content/train_data/valid/99/clap_estimate.wav\"  # Your separated file\n",
        "output_wav = \"/content/train_data/valid/99/clap_cleaned.wav\"\n",
        "\n",
        "# Load audio\n",
        "y, sr = librosa.load(input_wav, sr=44100)\n",
        "\n",
        "###  Apply Wiener Filtering with Larger Kernel (Less Aggressive)\n",
        "y_denoised = scipy.signal.wiener(y, mysize=512)  # Reduced smoothing to keep details\n",
        "\n",
        "### Soft Bandpass Filtering (Preserve More Highs & Lows)\n",
        "nyquist = sr / 2\n",
        "low_cutoff = 100  # Lower cutoff to preserve more bass\n",
        "high_cutoff = 12000  # Higher cutoff to keep more high frequencies\n",
        "\n",
        "b, a = scipy.signal.butter(3, [low_cutoff / nyquist, high_cutoff / nyquist], btype='band')\n",
        "y_filtered = scipy.signal.filtfilt(b, a, y_denoised)\n",
        "\n",
        "### Reduce Noise Without Overdoing It\n",
        "y_reduced = nr.reduce_noise(y=y_filtered, sr=sr, prop_decrease=0.2, stationary=True)  # Lower noise reduction\n",
        "\n",
        "### Normalize Volume (Fix Low Output Levels)\n",
        "y_normalized = y_reduced / np.max(np.abs(y_reduced)) * 0.9  # Avoid clipping but keep loudness\n",
        "\n",
        "### Save Processed File\n",
        "sf.write(output_wav, y_normalized, sr)\n",
        "\n",
        "# Play the cleaned file\n",
        "print(f\" Cleaned file saved as {output_wav}\")\n",
        "ipd.display(ipd.Audio(output_wav, rate=sr))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "3Q6R1pcuvdgf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Load MUSDB18 dataset (test subset)\n",
        "musdb = nussl.datasets.MUSDB18(subsets=['test'], download=True)\n",
        "\n",
        "# Manually set this index to browse different samples\n",
        "sample_idx = 27  # Change this to listen to different samples\n",
        "\n",
        "# Load the sample\n",
        "item = musdb[sample_idx]\n",
        "mixture_audio = item['mix']\n",
        "sr = mixture_audio.sample_rate\n",
        "\n",
        "# Save the sample for playback\n",
        "test_sample_path = \"/content/musdb18_sample.wav\"\n",
        "sf.write(test_sample_path, mixture_audio.audio_data.T, sr)\n",
        "\n",
        "# 🎧 Play the sample\n",
        "print(f\" Now playing sample {sample_idx}:\")\n",
        "ipd.display(ipd.Audio(test_sample_path, rate=sr))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "kC1o-kkKvfcp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Convert audio to PyTorch tensor and move to the correct device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "audio_tensor = torch.tensor(mixture_audio.audio_data, dtype=torch.float32).to(device)\n",
        "\n",
        "# Ensure correct model paths\n",
        "model_dir = \"/content/open-unmix-pytorch/scripts/open-unmix/\"  # Path to trained model\n",
        "model_name = \"clap\"  # The target you trained for\n",
        "\n",
        "# Load separator model\n",
        "print(f\" Loading separator for model: {model_name} on {device}\")\n",
        "separator = utils.load_separator(\n",
        "    model_str_or_path=model_dir,\n",
        "    targets=[model_name],\n",
        "    niter=1,\n",
        "    residual=True,  # Enable residual to avoid single target issues\n",
        "    wiener_win_len=300,\n",
        "    device=device,\n",
        "    pretrained=True,\n",
        "    filterbank=\"torch\",\n",
        ")\n",
        "\n",
        "separator.to(device).freeze()\n",
        "print(f\" Separator loaded successfully!\")\n",
        "\n",
        "# Run model on the mixture\n",
        "estimates = predict.separate(\n",
        "    audio=audio_tensor,\n",
        "    rate=sr,\n",
        "    separator=separator,\n",
        "    targets=[model_name],\n",
        ")\n",
        "\n",
        "# Check if output exists\n",
        "if model_name not in estimates:\n",
        "    print(f\" No '{model_name}' estimate was generated!\")\n",
        "else:\n",
        "    # Extract separated claps\n",
        "    clap_estimate = np.squeeze(estimates[model_name].cpu().numpy())\n",
        "\n",
        "    # Save the separated claps\n",
        "    output_path = \"/content/musdb18_clap_estimate.wav\"\n",
        "    sf.write(output_path, clap_estimate.T, sr)\n",
        "\n",
        "    print(f\"Separated claps saved at: {output_path}\")\n",
        "\n",
        "    # Play the separated claps\n",
        "    print(\"\\n Separated Claps (Model Output):\")\n",
        "    ipd.display(ipd.Audio(output_path, rate=sr))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "Zc_XV_JtvgzS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Load the separated clap audio\n",
        "clap_path = \"/content/musdb18_clap_estimate.wav\"\n",
        "y, sr = librosa.load(clap_path, sr=44100)\n",
        "\n",
        "# Apply noise reduction\n",
        "reduced_noise = nr.reduce_noise(y=y, sr=sr, prop_decrease=0.7)  # Tune `prop_decrease` if needed\n",
        "\n",
        "# Apply dynamic range compression\n",
        "compressed = librosa.effects.percussive(reduced_noise)\n",
        "\n",
        "# Normalize the audio\n",
        "normalized = librosa.util.normalize(compressed)\n",
        "\n",
        "# Save the post-processed audio\n",
        "processed_path = \"/content/musdb18_clap_estimate_processed.wav\"\n",
        "sf.write(processed_path, normalized, sr)\n",
        "\n",
        "# Play the post-processed audio\n",
        "print(\"\\n Post-processed Claps (Denoised & Enhanced):\")\n",
        "ipd.display(ipd.Audio(processed_path, rate=sr))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "gJcOa55zvjrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After clap\n"
      ],
      "metadata": {
        "id": "qFkKHxxCPOju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import shutil\n",
        "import gc\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# --- FIXED FUNCTIONS ---\n",
        "def loop_audio_to_duration(audio, target_length):\n",
        "    \"\"\"Ensure audio is exactly target_length samples\"\"\"\n",
        "    if audio.ndim == 1:\n",
        "        current_length = len(audio)\n",
        "        if current_length < target_length:\n",
        "            repeats = (target_length // current_length) + 1\n",
        "            return np.tile(audio, repeats)[:target_length]\n",
        "        return audio[:target_length]\n",
        "    else:\n",
        "        current_length = audio.shape[1]\n",
        "        if current_length < target_length:\n",
        "            repeats = (target_length // current_length) + 1\n",
        "            return np.tile(audio, (1, repeats))[:, :target_length]\n",
        "        return audio[:, :target_length]\n",
        "\n",
        "def to_stereo(audio):\n",
        "    \"\"\"Convert mono to stereo (samples, 2)\"\"\"\n",
        "    return np.column_stack([audio, audio]) if audio.ndim == 1 else audio.T if audio.shape[0] == 2 else audio\n",
        "\n",
        "def augment_audio(audio, sr):\n",
        "    \"\"\"Fixed time_stretch call with rate= parameter\"\"\"\n",
        "    if np.random.rand() < 0.5:\n",
        "        steps = random.choice([-2, -1, 1, 2])\n",
        "        audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=steps)\n",
        "    if np.random.rand() < 0.5:\n",
        "        rate = random.uniform(0.85, 1.15)\n",
        "        audio = librosa.effects.time_stretch(audio, rate=rate)  # Corrected line\n",
        "    return audio\n",
        "\n",
        "# --- ENHANCED GENERATION ---\n",
        "def generate_dataset():\n",
        "    # Configuration\n",
        "    input_path = \"/content/drive/MyDrive/Drum_Loops\"\n",
        "    output_path = \"/content/train_data_v2\"\n",
        "    sr = 44100\n",
        "    seq_dur = 7.75  # Must match training\n",
        "    max_samples = int(seq_dur * sr)\n",
        "\n",
        "    # Clean setup\n",
        "    shutil.rmtree(output_path, ignore_errors=True)\n",
        "    os.makedirs(os.path.join(output_path, \"train\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_path, \"valid\"), exist_ok=True)\n",
        "\n",
        "    # Load instrument files\n",
        "    instrument_files = {}\n",
        "    categories = [\"percussion\", \"kick\", \"hihat\", \"harp\", \"flute\",\n",
        "                \"clarinet\", \"clap\", \"choir\", \"cello\", \"acousticGuitar\"]\n",
        "\n",
        "    for cat in categories:\n",
        "        cat_path = os.path.join(input_path, cat)\n",
        "        instrument_files[cat] = librosa.util.find_files(cat_path) if os.path.exists(cat_path) else []\n",
        "        print(f\"Found {len(instrument_files[cat])} {cat} files\")\n",
        "\n",
        "    # Generate 300 samples with Demucs integration\n",
        "    for i in range(1, 301):\n",
        "        try:\n",
        "            # Create original mixture\n",
        "            track_dir = os.path.join(output_path, \"train\" if i <= 250 else \"valid\", f\"{i:03d}\")\n",
        "            os.makedirs(track_dir, exist_ok=True)\n",
        "\n",
        "            # 1. Generate flute track\n",
        "            flute_file = random.choice(instrument_files[\"flute\"])\n",
        "            flute_audio, _ = librosa.load(flute_file, sr=sr, mono=True)\n",
        "            flute_audio = augment_audio(flute_audio, sr)\n",
        "            flute_audio = loop_audio_to_duration(flute_audio, max_samples)\n",
        "            sf.write(os.path.join(track_dir, \"flute.wav\"), to_stereo(flute_audio), sr)\n",
        "\n",
        "            # 2. Create mixture\n",
        "            other_cats = [c for c in categories if c != \"flute\" and instrument_files[c]]\n",
        "            selected = random.sample(other_cats, random.randint(2, 5))\n",
        "\n",
        "            mixture = 0.7 * flute_audio\n",
        "            for cat in selected:\n",
        "                audio_file = random.choice(instrument_files[cat])\n",
        "                audio, _ = librosa.load(audio_file, sr=sr, mono=True)\n",
        "                audio = augment_audio(audio, sr)\n",
        "                audio = loop_audio_to_duration(audio, max_samples)\n",
        "                mixture += 0.3 * audio\n",
        "\n",
        "            # Save original mixture\n",
        "            sf.write(os.path.join(track_dir, \"mixture_original.wav\"), to_stereo(mixture), sr)\n",
        "\n",
        "            # 3. Process with Demucs\n",
        "            subprocess.run([\n",
        "                \"demucs\",\n",
        "                \"--two-stems\", \"other\",\n",
        "                os.path.join(track_dir, \"mixture_original.wav\"),\n",
        "                \"-o\", track_dir\n",
        "            ], check=True)\n",
        "\n",
        "            # 4. Replace mixture with Demucs' output\n",
        "            demucs_mix = os.path.join(track_dir, \"htdemucs\", \"mixture_original\", \"other.wav\")\n",
        "            os.rename(demucs_mix, os.path.join(track_dir, \"mixture.wav\"))\n",
        "\n",
        "            # Cleanup\n",
        "            shutil.rmtree(os.path.join(track_dir, \"htdemucs\"))\n",
        "            os.remove(os.path.join(track_dir, \"mixture_original.wav\"))\n",
        "\n",
        "            print(f\"[{i}] Generated track {track_dir}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipped track {i}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(\"✅ Dataset generated with Demucs pre-processing!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_dataset()\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "baTLkVNvPPzQ",
        "outputId": "ffc1352f-739c-4081-bd74-e3851f182087"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!python open-unmix-pytorch/scripts/train.py \\\n",
        "    --dataset aligned \\\n",
        "    --root /content/train_data_v2 \\\n",
        "    --input-file mixture.wav \\\n",
        "    --output-file flute.wav \\\n",
        "    --target flute \\\n",
        "    --seq-dur 7.74 \\\n",
        "    --nfft 4096 \\\n",
        "    --nhop 1024 \\\n",
        "    --epochs 30 \\\n",
        "    --batch-size 16 \\\n",
        "    --hidden-size 512 \\\n",
        "    --lr 0.001 \\\n",
        "    --weight-decay 0.0001\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nUIwYhzqT_WW",
        "outputId": "da882794-9c23-432f-cb15-66f145b9aa21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from openunmix import predict\n",
        "from openunmix import utils\n",
        "\n",
        "def process_full_song(input_path, output_path, model_dir, target_name=\"flute\", max_duration=None):\n",
        "    # Step 1: Run Demucs to get 'other' stem\n",
        "    demucs_output_dir = os.path.join(\"separated\", \"htdemucs\")\n",
        "    base_name = Path(input_path).stem\n",
        "\n",
        "    # Run Demucs command\n",
        "    subprocess.run([\n",
        "        \"demucs\",\n",
        "        \"--two-stems\", \"other\",\n",
        "        \"-n\", \"htdemucs\",\n",
        "        input_path,\n",
        "        \"-o\", demucs_output_dir\n",
        "    ], check=True)\n",
        "\n",
        "    # Step 2: Load Demucs' 'other' stem\n",
        "    # Update the Demucs output path handling\n",
        "    other_stem_path = os.path.join(\n",
        "        demucs_output_dir,\n",
        "        \"htdemucs\",\n",
        "        Path(input_path).with_suffix(\"\").name,  # Handles spaces/special chars\n",
        "        \"other.wav\"\n",
        "    )\n",
        "\n",
        "    ipd.display(ipd.Audio(other_stem_path, rate=44100))\n",
        "    y, sr = librosa.load(other_stem_path, sr=44100, mono=False)\n",
        "\n",
        "    if y.ndim == 1:\n",
        "        y = np.vstack([y, y])  # Ensure stereo\n",
        "\n",
        "    # Apply duration limit if specified\n",
        "    if max_duration:\n",
        "        max_samples = int(max_duration * sr)\n",
        "        y = y[:, :max_samples]\n",
        "\n",
        "    # Step 3: Initialize separator\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    separator = utils.load_separator(\n",
        "        model_str_or_path=model_dir,\n",
        "        targets=[target_name],\n",
        "        niter=0,\n",
        "        residual=True,\n",
        "        device=device,\n",
        "        pretrained=False\n",
        "    )\n",
        "\n",
        "    # Load custom weights\n",
        "    state_dict = torch.load(f\"{model_dir}/{target_name}.pth\", map_location=device)\n",
        "    adjusted_state_dict = {f\"target_models.{target_name}.{k}\": v for k, v in state_dict.items()}\n",
        "    separator.load_state_dict(adjusted_state_dict, strict=False)\n",
        "    separator.to(device).eval()\n",
        "\n",
        "    # Step 4: Process in chunks\n",
        "    chunk_size = int(7.74 * sr)\n",
        "    estimates = []\n",
        "\n",
        "    for i in range(0, y.shape[1], chunk_size):\n",
        "        chunk = y[:, i:i+chunk_size]\n",
        "        audio_tensor = torch.tensor(chunk, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            estimates_chunk = predict.separate(\n",
        "                audio=audio_tensor,\n",
        "                rate=sr,\n",
        "                separator=separator,\n",
        "                targets=[target_name]\n",
        "            )\n",
        "            flute_estimate = estimates_chunk[target_name].squeeze().cpu().numpy().T\n",
        "\n",
        "        estimates.append(flute_estimate)\n",
        "\n",
        "    # Step 5: Combine and save\n",
        "    full_estimate = np.concatenate(estimates, axis=0)\n",
        "    sf.write(output_path, full_estimate, sr)\n",
        "\n",
        "    # Cleanup Demucs output\n",
        "    shutil.rmtree(os.path.join(demucs_output_dir, \"htdemucs\", base_name))\n",
        "\n",
        "    print(f\"Flute track saved to {output_path}\")\n",
        "\n",
        "# Usage\n",
        "process_full_song(\n",
        "    input_path=\"/content/separated/htdemucs/htdemucs/test.wav\",\n",
        "    output_path=\"flute_enhanced.wav\",\n",
        "    model_dir=\"/content/open-unmix/\"\n",
        "    # max_duration=7.74  # Remove for full song\n",
        ")\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "eOdsbi-rJd9T",
        "outputId": "87ff77ee-bd87-4362-8ab9-6da79340d954"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import IPython.display as ipd\n",
        "import os\n",
        "\n",
        "flute_path = \"flute_enhanced.wav\"\n",
        "other_stem_path = os.path.join(\"separated\", \"htdemucs\", \"test\", \"other.wav\")\n",
        "\n",
        "if os.path.exists(flute_path):\n",
        "    print(\"Flute Enhanced Audio:\")\n",
        "    ipd.display(ipd.Audio(flute_path, rate=44100))\n",
        "else:\n",
        "    print(\"Flute track generation failed - check model training\")\n",
        "\n",
        "if os.path.exists(other_stem_path):\n",
        "    print(\"\\nDemucs 'Other' Stem:\")\n",
        "    ipd.display(ipd.Audio(other_stem_path, rate=44100))\n",
        "else:\n",
        "    print(\"Demucs failed - check input file permissions\")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "N38yBN_92UBJ",
        "outputId": "030ebf68-4f9d-405b-83c2-fabe94575f43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BabySlakh"
      ],
      "metadata": {
        "id": "sWOLwqj1A_Hl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import yaml\n",
        "import tarfile\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Extract the tar.gz file if needed\n",
        "def extract_archive(archive_path, extract_path):\n",
        "    if not os.path.exists(extract_path):\n",
        "        print(f\"Extracting {archive_path} to {extract_path}...\")\n",
        "        with tarfile.open(archive_path, 'r:gz') as tar:\n",
        "            tar.extractall(path=extract_path)\n",
        "        print(\"Extraction complete!\")\n",
        "    else:\n",
        "        print(f\"Using existing directory: {extract_path}\")\n",
        "\n",
        "# Step 2: Analyze the dataset\n",
        "def analyze_babyslakh(dataset_path):\n",
        "    # Initialize counters\n",
        "    inst_class_counter = Counter()\n",
        "    program_name_counter = Counter()\n",
        "    plugin_name_counter = Counter()\n",
        "    is_drum_counter = Counter()\n",
        "\n",
        "    # Track additional information\n",
        "    track_count = 0\n",
        "    stems_per_track = []\n",
        "\n",
        "    # Walk through all directories\n",
        "    print(f\"Scanning dataset at: {dataset_path}\")\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        if 'metadata.yaml' in files:\n",
        "            track_count += 1\n",
        "            metadata_path = os.path.join(root, 'metadata.yaml')\n",
        "\n",
        "            try:\n",
        "                with open(metadata_path, 'r') as f:\n",
        "                    metadata = yaml.safe_load(f)\n",
        "\n",
        "                # Skip if no stems data\n",
        "                if 'stems' not in metadata:\n",
        "                    continue\n",
        "\n",
        "                # Count stems in this track\n",
        "                stems_per_track.append(len(metadata['stems']))\n",
        "\n",
        "                # Analyze each stem\n",
        "                for stem_id, stem_data in metadata['stems'].items():\n",
        "                    if 'inst_class' in stem_data:\n",
        "                        inst_class_counter[stem_data['inst_class']] += 1\n",
        "                    if 'midi_program_name' in stem_data:\n",
        "                        program_name_counter[stem_data['midi_program_name']] += 1\n",
        "                    if 'plugin_name' in stem_data:\n",
        "                        plugin_name_counter[stem_data['plugin_name']] += 1\n",
        "                    if 'is_drum' in stem_data:\n",
        "                        is_drum_counter[stem_data['is_drum']] += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {metadata_path}: {e}\")\n",
        "\n",
        "    return {\n",
        "        'track_count': track_count,\n",
        "        'stems_per_track': stems_per_track,\n",
        "        'inst_class': inst_class_counter,\n",
        "        'program_name': program_name_counter,\n",
        "        'plugin_name': plugin_name_counter,\n",
        "        'is_drum': is_drum_counter\n",
        "    }\n",
        "\n",
        "# Step 3: Visualize results\n",
        "def visualize_results(results):\n",
        "    # Print overall statistics\n",
        "    print(f\"\\nTotal tracks analyzed: {results['track_count']}\")\n",
        "    print(f\"Average stems per track: {sum(results['stems_per_track'])/len(results['stems_per_track']):.2f}\")\n",
        "    print(f\"Min stems: {min(results['stems_per_track'])}, Max stems: {max(results['stems_per_track'])}\")\n",
        "\n",
        "    # Create pandas DataFrames for better display\n",
        "    print(\"\\n--- Top 10 Instrument Classes ---\")\n",
        "    inst_df = pd.DataFrame(results['inst_class'].most_common(), columns=['Instrument Class', 'Count'])\n",
        "    print(inst_df.head(10))\n",
        "\n",
        "    print(\"\\n--- Top 10 Specific Instruments ---\")\n",
        "    prog_df = pd.DataFrame(results['program_name'].most_common(), columns=['Instrument', 'Count'])\n",
        "    print(prog_df.head(10))\n",
        "\n",
        "    print(\"\\n--- Drum vs Non-Drum Instruments ---\")\n",
        "    drum_df = pd.DataFrame(results['is_drum'].most_common(), columns=['Is Drum', 'Count'])\n",
        "    print(drum_df)\n",
        "\n",
        "    # Visualization of top instrument classes\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    top_classes = dict(results['inst_class'].most_common(10))\n",
        "    plt.bar(top_classes.keys(), top_classes.values())\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.title('Top 10 Instrument Classes')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('instrument_classes.png')\n",
        "\n",
        "    # Visualization of top specific instruments\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    top_instruments = dict(results['program_name'].most_common(10))\n",
        "    plt.bar(top_instruments.keys(), top_instruments.values())\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.title('Top 10 Specific Instruments')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('specific_instruments.png')\n",
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    # Set your paths\n",
        "    archive_path = '/content/drive/MyDrive/babyslakh_16k.tar.gz'\n",
        "    extract_path = '/content/drive/MyDrive/babyslakh_16k'\n",
        "\n",
        "    # Extract if needed\n",
        "    try:\n",
        "        extract_archive(archive_path, extract_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Extraction error: {e}\")\n",
        "        print(\"Continuing with analysis assuming the dataset is already extracted...\")\n",
        "\n",
        "    # Analyze the dataset\n",
        "    results = analyze_babyslakh(extract_path)\n",
        "\n",
        "    # Display and visualize results\n",
        "    visualize_results(results)\n",
        "\n",
        "    # Save results to CSV for further analysis\n",
        "    pd.DataFrame(results['inst_class'].most_common(), columns=['Instrument Class', 'Count']).to_csv('instrument_classes.csv')\n",
        "    pd.DataFrame(results['program_name'].most_common(), columns=['Instrument', 'Count']).to_csv('specific_instruments.csv')\n",
        "\n",
        "    print(\"\\nAnalysis complete! CSV files and plots have been saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Uf3YLzguBDVE",
        "outputId": "ac76f32c-d233-4c27-d13c-b4f1866b42e5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import torch\n",
        "from torchaudio.transforms import Resample\n",
        "\n",
        "# --- OPEN-UNMIX COMPLIANT PARAMETERS ---\n",
        "TARGET_SR = 44100        # Mandatory sample rate[1][7]\n",
        "NFFT = 4096              # STFT size (cannot be changed)[1][4]\n",
        "NHOP = 1024              # STFT hop size[1][4]\n",
        "SEQ_DUR = 6.1            # Default training duration[1][7]\n",
        "BATCH_SIZE = 16          # Default batch size[7]\n",
        "NB_CHANNELS = 2          # Force stereo input[4]\n",
        "\n",
        "def create_dataset():\n",
        "    # Clean existing data\n",
        "    train_data_path = \"/content/train_data\"\n",
        "    if os.path.exists(train_data_path):\n",
        "        shutil.rmtree(train_data_path)\n",
        "        print(\"Removed existing training data\")\n",
        "\n",
        "    # Create directory structure\n",
        "    os.makedirs(os.path.join(train_data_path, \"train\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(train_data_path, \"valid\"), exist_ok=True)\n",
        "\n",
        "    # Initialize resampler for 16kHz→44.1kHz conversion\n",
        "    resampler = Resample(\n",
        "        orig_freq=16000,\n",
        "        new_freq=TARGET_SR,\n",
        "        resampling_method=\"sinc_interp_kaiser\"\n",
        "    )\n",
        "\n",
        "    # Find all Choir Aahs tracks\n",
        "    choir_tracks = []\n",
        "    for root, _, files in os.walk(\"/content/drive/MyDrive/babyslakh_16k\"):\n",
        "        if \"metadata.yaml\" in files:\n",
        "            with open(os.path.join(root, \"metadata.yaml\")) as f:\n",
        "                metadata = yaml.safe_load(f)\n",
        "\n",
        "            for stem_id, stem_data in metadata.get(\"stems\", {}).items():\n",
        "                if stem_data.get(\"midi_program_name\", \"\").lower() == \"choir aahs\":\n",
        "                    stem_path = os.path.join(root, \"stems\", f\"{stem_id}.wav\")\n",
        "                    if os.path.exists(stem_path):\n",
        "                        choir_tracks.append((root, stem_path))\n",
        "                        break\n",
        "\n",
        "    # Split tracks (2 validation, rest training)\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(choir_tracks)\n",
        "    valid_tracks = choir_tracks[:2]\n",
        "    train_tracks = choir_tracks[2:]\n",
        "\n",
        "    # Audio processing parameters\n",
        "    max_samples = int(SEQ_DUR * TARGET_SR)  # 6s * 44100 = 264600 samples\n",
        "\n",
        "    def process_and_save(track_path, stem_path, dest_dir):\n",
        "        \"\"\"Process audio to meet Open-Unmix requirements\"\"\"\n",
        "        os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "        # Process mixture (full track mix)\n",
        "        mix_wave, _ = torchaudio.load(os.path.join(track_path, \"mix.wav\"))\n",
        "        mix_wave = resampler(mix_wave)\n",
        "        mix_wave = mix_wave[:, :max_samples]  # Exact 6s duration\n",
        "        if mix_wave.shape[0] == 1:  # Force stereo\n",
        "            mix_wave = torch.cat([mix_wave, mix_wave], dim=0)\n",
        "        torchaudio.save(\n",
        "            os.path.join(dest_dir, \"mixture.wav\"),\n",
        "            mix_wave,\n",
        "            TARGET_SR\n",
        "        )\n",
        "\n",
        "        # Process target stem\n",
        "        stem_wave, _ = torchaudio.load(stem_path)\n",
        "        stem_wave = resampler(stem_wave)\n",
        "        stem_wave = stem_wave[:, :max_samples]\n",
        "        if stem_wave.shape[0] == 1:\n",
        "            stem_wave = torch.cat([stem_wave, stem_wave], dim=0)\n",
        "        torchaudio.save(\n",
        "            os.path.join(dest_dir, \"choir_aahs.wav\"),\n",
        "            stem_wave,\n",
        "            TARGET_SR\n",
        "        )\n",
        "\n",
        "    # Generate training data\n",
        "    for i, (track_path, stem_path) in enumerate(train_tracks):\n",
        "        dest_dir = os.path.join(train_data_path, \"train\", f\"track_{i:03d}\")\n",
        "        process_and_save(track_path, stem_path, dest_dir)\n",
        "\n",
        "    # Generate validation data\n",
        "    for i, (track_path, stem_path) in enumerate(valid_tracks):\n",
        "        dest_dir = os.path.join(train_data_path, \"valid\", f\"track_{i:03d}\")\n",
        "        process_and_save(track_path, stem_path, dest_dir)\n",
        "\n",
        "    print(f\"\"\"\n",
        "    Dataset successfully created!\n",
        "    - Training tracks: {len(train_tracks)}\n",
        "    - Validation tracks: {len(valid_tracks)}\n",
        "    - Sample rate: {TARGET_SR}Hz\n",
        "    - Sequence duration: {SEQ_DUR}s\n",
        "    - STFT parameters: nfft={NFFT}, nhop={NHOP}\n",
        "    \"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_dataset()\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "aY_BgQ-yCXnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c555f558-22f4-4cc7-d364-dcb3cb7b285c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!python open-unmix-pytorch/scripts/train.py \\\n",
        "    --dataset aligned \\\n",
        "    --root /content/train_data \\\n",
        "    --input-file mixture.wav \\\n",
        "    --output-file choir_aahs.wav \\\n",
        "    --seq-dur 6.0 \\\n",
        "    --nfft 4096 \\\n",
        "    --nhop 1024 \\\n",
        "    --nb-channels 2 \\\n",
        "    --batch-size 16 \\\n",
        "    --epochs 50 \\\n",
        "    --patience 140"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1XBc-i2Pep8",
        "outputId": "1c9b711a-c8e3-47fb-bfcc-5a3660dbf61e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import librosa\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from openunmix import predict, utils\n",
        "\n",
        "# Configuration\n",
        "TARGET_NAME = \"choir_aahs\"\n",
        "MODEL_DIR = \"/content/open-unmix/\"  # Contains choir_aahs.pth and choir_aahs.json\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load mixture audio with proper channel handling\n",
        "mixture_path = \"/content/train_data/valid/track_000/mixture.wav\"\n",
        "y, sr = librosa.load(mixture_path, sr=44100, mono=False)\n",
        "\n",
        "# Convert to PyTorch tensor with correct shape (channels, samples)\n",
        "if y.ndim == 1:  # Convert mono to stereo\n",
        "    y = np.stack([y, y], axis=0)\n",
        "audio_tensor = torch.tensor(y, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Load separator with proper configuration (from Open-Unmix docs[1][6])\n",
        "print(f\"Loading separator for {TARGET_NAME} on {DEVICE}\")\n",
        "separator = utils.load_separator(\n",
        "    model_str_or_path=MODEL_DIR,\n",
        "    targets=[TARGET_NAME],\n",
        "    niter=1,\n",
        "    residual=False,  # Disable residual for single-target separation[1][9]\n",
        "    wiener_win_len=300,\n",
        "    device=DEVICE,\n",
        "    pretrained=True,\n",
        "    filterbank=\"torch\",\n",
        ")\n",
        "separator.freeze()\n",
        "separator.to(DEVICE)\n",
        "\n",
        "# Perform separation with proper device alignment\n",
        "print(f\"Separating {TARGET_NAME}...\")\n",
        "# Updated separation call\n",
        "estimates = predict.separate(\n",
        "    audio=audio_tensor,\n",
        "    rate=sr,\n",
        "    targets=[TARGET_NAME],\n",
        "    model_dir=MODEL_DIR,  # Directory containing choir_aahs.pth/choir_aahs.json\n",
        "    niter=0,  # Disable EM for single target\n",
        "    residual=False,\n",
        "    device=DEVICE\n",
        ")\n",
        "\n",
        "# Handle output with proper shape validation\n",
        "if TARGET_NAME not in estimates:\n",
        "    raise ValueError(f\"No '{TARGET_NAME}' estimate generated!\")\n",
        "\n",
        "choir_estimate = estimates[TARGET_NAME].cpu().numpy()\n",
        "\n",
        "# Remove batch dimension if present (batch_size=1)\n",
        "if choir_estimate.ndim == 3:\n",
        "    choir_estimate = choir_estimate.squeeze(0)\n",
        "\n",
        "# Save output with proper channel order\n",
        "output_path = f\"/content/train_data/valid/track_000/{TARGET_NAME}_estimate.wav\"\n",
        "sf.write(output_path, choir_estimate.T, sr)  # Transpose to (samples, channels)\n",
        "\n",
        "print(f\"Successfully saved separation to {output_path}\")\n",
        "\n",
        "# Audio playback\n",
        "from IPython.display import Audio\n",
        "print(f\"\\nOriginal Mixture:\")\n",
        "display(Audio(mixture_path, rate=sr))\n",
        "print(f\"\\nSeparated {TARGET_NAME}:\")\n",
        "display(Audio(output_path, rate=sr))\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "RzXIs0S_Y8lr",
        "outputId": "03257a77-49b8-4ad4-9537-a325f6f45a2e"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}